{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39160,
     "status": "ok",
     "timestamp": 1596110297445,
     "user": {
      "displayName": "Katy Brown",
      "photoUrl": "",
      "userId": "04725673548216289527"
     },
     "user_tz": -60
    },
    "id": "_d_1WZWg2nCE",
    "outputId": "3f29b4cf-57f8-4d8b-d6e0-75f83cfdaeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libgeos-dev is already the newest version (3.6.2-1build2).\n",
      "libproj-dev is already the newest version (4.9.3-2).\n",
      "proj-bin is already the newest version (4.9.3-2).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-440\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
      "  Building wheel for basemap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install -q geopandas\n",
    "!apt install -q proj-bin libproj-dev libgeos-dev -y\n",
    "!pip install -q https://github.com/matplotlib/basemap/archive/master.zip\n",
    "\n",
    "# Pandas is a package containing additional functions to use data frames in Python\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# These two lines allow the notebook to access the Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# This is the path to the project folder within the Google Drive.\n",
    "file_path = \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zM0yVXe_GG55"
   },
   "source": [
    "## Georeferencing - Automated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_REbnNmWa-E"
   },
   "source": [
    "Now we can run the georeferencing steps on all the tables.  We can automate this by reading all the species names from a file (in our Drive as species_names.tsv) into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5UMLXSVWZ83"
   },
   "outputs": [],
   "source": [
    "species_list = [line.strip() for line in open(file_path + \"species_names.tsv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Suu258UHBAIC"
   },
   "source": [
    "We can use a variable to control the species name - this means we only have to change it in one place every time we want to run a different table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1596110305987,
     "user": {
      "displayName": "Katy Brown",
      "photoUrl": "",
      "userId": "04725673548216289527"
     },
     "user_tz": -60
    },
    "id": "wmv00W4jA8lO",
    "outputId": "de8dff35-b3a4-4020-d445-cb83bc2e0d56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acromyrmex_echinatior',\n",
       " 'Amblyomma_americanum',\n",
       " 'Amblyomma_aureolatum',\n",
       " 'Amblyomma_sculptum',\n",
       " 'Apis_cerana',\n",
       " 'Apis_mellifera',\n",
       " 'Asobara_tabida',\n",
       " 'Athalia_rosae',\n",
       " 'Biorhiza_pallida',\n",
       " 'Bombus_terrestris',\n",
       " 'Camponotus_castaneus',\n",
       " 'Camponotus_floridanus',\n",
       " 'Camponotus_japonicus',\n",
       " 'Camponotus_ligniperdus',\n",
       " 'Cardiocondyla_obscurior',\n",
       " 'Cephus_cinctus',\n",
       " 'Ceratina_australensis',\n",
       " 'Cotesia_vestalis',\n",
       " 'Crematogaster_osakensis',\n",
       " 'Dermacentor_andersoni',\n",
       " 'Dermacentor_variabilis',\n",
       " 'Dinoponera_quadriceps',\n",
       " 'Exoneurella_tridentata',\n",
       " 'Fopius_arisanus',\n",
       " 'Halictus_scabiosae',\n",
       " 'Harpegnathos_saltator',\n",
       " 'Ixodes_persulcatus',\n",
       " 'Ixodes_ricinus',\n",
       " 'Ixodes_scapularis',\n",
       " 'Lasius_niger',\n",
       " 'Linepithema_humile',\n",
       " 'Lysiphlebus_fabarum',\n",
       " 'Megachile_rotundata',\n",
       " 'Megalopta_genalis',\n",
       " 'Megastigmus_spermotrophus',\n",
       " 'Messor_barbarus',\n",
       " 'Messor_capitatus',\n",
       " 'Messor_hellenius',\n",
       " 'Messor_structor',\n",
       " 'Microplitis_demolitor',\n",
       " 'Monomorium_pharaonis',\n",
       " 'Nasonia_giraulti',\n",
       " 'Nasonia_vitripennis',\n",
       " 'Neodiprion_lecontei',\n",
       " 'Ooceraea_biroi',\n",
       " 'Osmia_bicornis',\n",
       " 'Osmia_cornuta',\n",
       " 'Panonychus_citri',\n",
       " 'Pheidole_pallidula',\n",
       " 'Pogonomyrmex_barbatus',\n",
       " 'Pogonomyrmex_californicus',\n",
       " 'Polistes_canadensis',\n",
       " 'Polistes_fuscatus',\n",
       " 'Polistes_metricus',\n",
       " 'Rhipicephalus_appendiculatus',\n",
       " 'Rhipicephalus_microplus',\n",
       " 'Rhipicephalus_sanguineus',\n",
       " 'Rhipicephalus_zambeziensis',\n",
       " 'Rhizoglyphus_robini',\n",
       " 'Solenopsis_invicta',\n",
       " 'Spalangia_endius',\n",
       " 'Temnothorax_ambiguus',\n",
       " 'Temnothorax_curvispinosus',\n",
       " 'Temnothorax_duloticus',\n",
       " 'Temnothorax_longispinosus',\n",
       " 'Temnothorax_pilagens',\n",
       " 'Tetranychus_cinnabarinus',\n",
       " 'Tetranychus_urticae',\n",
       " 'Trichogramma_chilonis',\n",
       " 'Trichogramma_pretiosum',\n",
       " 'Trichomalopsis_sarcophagae',\n",
       " 'Varroa_destructor',\n",
       " 'Varroa_jacobsoni',\n",
       " 'Wasmannia_auropunctata']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gXvKaTc3EiM"
   },
   "source": [
    "Read the data table which we created in the data cleaning step into Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9C1fwVAW8kC"
   },
   "source": [
    "Using a ```for``` loop we can go through each of these names one by one and run all the georeferencing steps.\n",
    "\n",
    "This is exactly the same code as we used for a single table - I have just moved it all into one cell to make it easier to run the loop.\n",
    "\n",
    "I have added two extra lines so that we can combine all the single row summary tables into one long table for all species.\n",
    "\n",
    "I also changed the order slightly to make it run faster and make the points on the map a bit smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJowWXj73G60"
   },
   "outputs": [],
   "source": [
    "# We will use the country polygons every time - we only need to import them once\n",
    "# Import polygons representing the shape and location of each country and\n",
    "# match these to the country names or codes.\n",
    "# The geometry column of this table contains the information needed to reproduce the shapes of the countries on a map.\n",
    "worldmap = gpd.read_file(file_path + \"/country_boundaries_shapefiles/World_Countries__Generalized_.shp\")\n",
    "worldmap = worldmap.to_crs('epsg:4088')\n",
    "\n",
    "# Record all the country polygons into a Python dictionary.  This code just links each country code to a polygon in the worldmap table.\n",
    "country_dict = dict()\n",
    "for iso, polygon in zip(worldmap['ISO'], worldmap['geometry']):\n",
    "  country_dict[iso] = polygon\n",
    "\n",
    "# create an empty DataFrame to store all the summary table rows\n",
    "all_geo_summary_tables = pd.DataFrame()\n",
    "\n",
    "# run everything for every species in the list\n",
    "for species_name in species_list:\n",
    "        # Everything in this indented section will run once for each species name\n",
    "\n",
    "        mytab = pd.read_csv(file_path + \"/filtered_main_tables/\" + species_name + \".csv\", sep=\"\\t\")\n",
    "        # Make another summary table to record the results of processing the data.\n",
    "        # Create an empty dataframe with these columns and with one row for this species\n",
    "        geosummary = pd.DataFrame(columns=['nrecords_after_data_cleaning', 'n_in_wrong_country', 'n_in_ocean',\n",
    "                                          'nrecords_final'],\n",
    "                              index=[species_name])\n",
    "\n",
    "        # record the original number of records\n",
    "        geosummary.loc[species_name, 'nrecords_after_data_cleaning'] = len(mytab)\n",
    "\n",
    "        # Merge the table with the file called “country_codes.tsv” .\n",
    "        # This will add an additional column - country name.\n",
    "\n",
    "        another_tab = pd.read_csv(file_path + \"country_codes.tsv\", sep=\"\\t\")\n",
    "        another_tab['countryCode'] = another_tab['countryCode'].str.strip()\n",
    "\n",
    "\n",
    "        # The \"ISO\" column in this table corresponds to the information in the \"countryCode\" column in our filtered species data table.\n",
    "\n",
    "        # We can use the map data to check that each point in the species table is actually in the country it is reported as being in.\n",
    "\n",
    "        # In order to allow Python to interpret geographical data, we need to convert our species dataframe into a \"geodataframe\".  This converts the points in the 'decimalLongitude' and 'decimalLatitude' columns into points on a map.\n",
    "        # In order to convert latitude and longitude points into map points we used a map projection - https://en.wikipedia.org/wiki/Map_projection - the points can be converted in different ways depending on the map used.\n",
    "        # Our species observation points and the country polygons are recorded in projection ESPG 4326 or the \"Web Mercator\" projection.\n",
    "\n",
    "        # convert the table to geopandas\n",
    "        # tell Python that \"decimalLongitude\" and \"decimalLatitude\" are geographical points in the EPSG 4326 projection.\n",
    "        gdf = gpd.GeoDataFrame(mytab,\n",
    "                              geometry=gpd.points_from_xy(mytab['decimalLongitude'], mytab['decimalLatitude']),\n",
    "                              crs=\"epsg:4326\")\n",
    "        # The table now has a \"geometry\" column with the new information.\n",
    "\n",
    "        # Converting both columns to a different projection - the \"World Equidistant Cylindrical\" or ESPG 4088 - just makes the images of the map look more familiar.\n",
    "        # We also add a couple of extra columns to the table here with the x and y positions in this projection - just for convenience later.\n",
    "        \n",
    "        gdf = gdf.to_crs('epsg:4088')\n",
    "\n",
    "        x_positions_cyl = [x.coords[0][0] for x in gdf['geometry']]\n",
    "        y_positions_cyl = [x.coords[0][1] for x in gdf['geometry']]\n",
    "        gdf['x_positions'] = x_positions_cyl\n",
    "        gdf['y_positions'] = y_positions_cyl\n",
    "\n",
    "\n",
    "        # We can now plot the points on a map.\n",
    "\n",
    "        # feel free to change these colours using the codes here: https://htmlcolorcodes.com/\n",
    "        sea_colour = '#B5F0FC'\n",
    "        land_colour = '#CDFCB5'\n",
    "        point_colour = '#8237B9'\n",
    "\n",
    "\n",
    "        # create an empty set of axis\n",
    "        f = plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # add a plot to these axis\n",
    "        a = f.add_subplot(111, facecolor=sea_colour)\n",
    "\n",
    "        # plot the countries onto the axis\n",
    "        worldmap.plot(ax=a, color=land_colour, edgecolor='black', lw=0.3)\n",
    "\n",
    "        # add the species observation points\n",
    "        a.scatter(gdf['x_positions'], gdf['y_positions'], s=10, marker=\"^\", color=point_colour)\n",
    "\n",
    "\n",
    "        # add a title\n",
    "        a.set_title(species_name.replace(\"_\", \" \") + \" Unfiltered Points\")\n",
    "\n",
    "        # save a copy\n",
    "        f.savefig(file_path + \"/unfiltered_maps/\" + species_name + \".png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        # We want to check that each point has been recorded in the right country.\n",
    "        # Using the ```within``` function we can check if this point is within the right polygon \n",
    "        # Now we can run through each point in the table and check it is in the right country.\n",
    "\n",
    "        correct_country = []\n",
    "        # run through every point the table\n",
    "        for iso, point in zip(gdf['countryCode'], gdf['geometry']):\n",
    "          if iso in country_dict:\n",
    "            # get the polygon the point should be inside\n",
    "            poly = country_dict[iso]\n",
    "            # check the point is in the polygon\n",
    "            correct_country.append(point.within(poly))\n",
    "          else:\n",
    "            correct_country.append(False)\n",
    "\n",
    "        # insert this information into the table\n",
    "        gdf['correct_country'] = correct_country\n",
    "\n",
    "        # Now the \"correct country\" column in the table tells us if the point was recorded in the right country or not.\n",
    "        # We want to save the incorrect countries in a separate table and count how many \n",
    "        # there are.\n",
    "        wrong_country = gdf[gdf['correct_country'] == False]\n",
    "        wrong_country.to_csv(file_path + \"/wrong_country_tables/\"  + species_name + \".csv\", sep=\"\\t\", index=None)\n",
    "\n",
    "        count_wrong_country = len(wrong_country)\n",
    "\n",
    "        gdf = gdf[gdf['correct_country'] == True]\n",
    "\n",
    "        geosummary.loc[species_name, \"n_in_wrong_country\"] = count_wrong_country\n",
    "\n",
    "        # Next we want to look for points which are in the ocean - these are unlikely to be correct, especially as they are far from the coast.\n",
    "        # We need to import a map projection from another package which provides a function to check this.\n",
    "\n",
    "        # read in a map in the world cylindrical projection\n",
    "        m = Basemap(projection='cyl',\n",
    "                    llcrnrlat=-90,\n",
    "                    urcrnrlat=90,\\\n",
    "                    llcrnrlon=-180,\n",
    "                    urcrnrlon=180,\n",
    "                    resolution='l')\n",
    "\n",
    "        # The ```is_land``` function tells us if the point is on land.\n",
    "        # We need to run this for every point in the table\n",
    "        # make a list to store the results\n",
    "        results = []\n",
    "        for long, lat in zip(gdf['decimalLongitude'], gdf['decimalLatitude']):\n",
    "          # check if the point is on land\n",
    "          point_is_land = m.is_land(long, lat)\n",
    "          results.append(point_is_land)\n",
    "\n",
    "        # put this data into the table\n",
    "        gdf['is_land'] = results\n",
    "\n",
    "        # Now we can move the \"sea\" points into a seperate table and filter them out of the main table.\n",
    "        gdf_sea = gdf[gdf['is_land'] == False]\n",
    "        geosummary.loc[species_name, 'n_in_ocean'] = len(gdf_sea)\n",
    "        gdf = gdf[gdf['is_land'] == True]\n",
    "\n",
    "        # Now we've finished filtering the table, we can make another map with just the good quality points.\n",
    "\n",
    "\n",
    "        # create an empty set of axis\n",
    "        f = plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # add a plot to these axis\n",
    "        a = f.add_subplot(111, facecolor=sea_colour)\n",
    "\n",
    "        # plot the countries onto the axis\n",
    "        worldmap.plot(ax=a, color=land_colour, edgecolor='black', lw=0.3)\n",
    "\n",
    "        # add the species observation points\n",
    "        a.scatter(gdf['x_positions'], gdf['y_positions'], s=10, marker=\"^\", color=point_colour)\n",
    "\n",
    "        # add a title\n",
    "        a.set_title(species_name.replace(\"_\", \" \") + \" Filtered Points\")\n",
    "\n",
    "        # save a copy\n",
    "        f.savefig(file_path + \"/filtered_maps/\" + species_name + \".png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        # We also want to record the remaining number of points in the summary table.\n",
    "        geosummary.loc[species_name, 'nrecords_final'] = len(gdf)\n",
    "\n",
    "        # Finally, we save the clean table and the summary table.\n",
    "        gdf.to_csv(file_path + \"/geo_filtered_main_tables/\" + species_name + \".csv\", sep=\"\\t\", index=None)\n",
    "        geosummary.to_csv(file_path + \"/geo_summary_tables/\" + species_name + \".csv\", sep=\"\\t\")\n",
    "\n",
    "        # add the summary to the big summary table\n",
    "        all_geo_summary_tables = all_geo_summary_tables.append(geosummary)\n",
    "\n",
    "# save the big summary table\n",
    "all_geo_summary_tables['species_name'] = all_geo_summary_tables.index.values\n",
    "all_geo_summary_tables.to_csv(file_path + \"final_summary_georeferencing.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytWZmLarcfGu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhGRHEPF8hT70EhbzLZ2VJ",
   "collapsed_sections": [],
   "name": "Georeferencing_Batch",
   "provenance": [
    {
     "file_id": "1rbirwHOr8xXXPDglGATrTMCv-2VduKhL",
     "timestamp": 1596106264643
    },
    {
     "file_id": "1JCagcN5sKQdf8OvgrxerPOcpDiTURygr",
     "timestamp": 1596019556776
    },
    {
     "file_id": "1vhDRNO8Ov_4I3V0wjyGdSv5qZd28P26R",
     "timestamp": 1596016780263
    },
    {
     "file_id": "1fgK0oP7jFP4z2krdH0pKzR--BtjpzyLs",
     "timestamp": 1595944079107
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
